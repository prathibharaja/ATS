{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIPuW/P8b3jF9LgPC60l5M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathibharaja/ATS/blob/main/Candidate_Filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()  # This unmounts the drive cleanly\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY1GamUV58tY",
        "outputId": "90d9d9ab-811c-4d34-f016-69643511462f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9kl8HCw5-Ib",
        "outputId": "0e3def5f-7f33-4946-ea0c-cc4a2fb54e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNIoSbqP5-Wk",
        "outputId": "2df0645b-e843-4dbf-d0b7-0f21e6b8f8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRk3wqpD5vO9",
        "outputId": "3342a947-2bf5-4fdf-ad24-c667bfc721d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Query A - Data Management Skills:\n",
            "  Document: Muhammad_Hamza_Insaf_Data_Analytics_CV.pdf, Score: 0.4080\n",
            "  Document: CV_Sukhorukykh.pdf, Score: 0.3489\n",
            "  Document: Erdem_Kurtulus_CV.pdf, Score: 0.3423\n",
            "  Document: Khayal Khudiyev Resume.pdf, Score: 0.2692\n",
            "  Document: 8421D6BC-69D7-476B-97D9-6C375F29AE86.pdf, Score: 0.1520\n",
            "  Document: Ekaterina Bochkova RESUME.pdf, Score: 0.1254\n",
            "  Document: CV_Adrian_Meneses-2025-APR-EN-3.pdf, Score: 0.1199\n",
            "  Document: [CV] H.P.P.A. Pemarathna.pdf, Score: 0.0476\n",
            "\n",
            "Results for Query B - Personal Attributes:\n",
            "  Document: CV_Sukhorukykh.pdf, Score: 0.1878\n",
            "  Document: Muhammad_Hamza_Insaf_Data_Analytics_CV.pdf, Score: 0.1532\n",
            "  Document: Khayal Khudiyev Resume.pdf, Score: 0.1434\n",
            "  Document: Erdem_Kurtulus_CV.pdf, Score: 0.1306\n",
            "  Document: CV_Adrian_Meneses-2025-APR-EN-3.pdf, Score: 0.1305\n",
            "  Document: [CV] H.P.P.A. Pemarathna.pdf, Score: 0.1181\n",
            "  Document: 8421D6BC-69D7-476B-97D9-6C375F29AE86.pdf, Score: 0.0971\n",
            "  Document: Ekaterina Bochkova RESUME.pdf, Score: 0.0697\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from google.colab import drive\n",
        "\n",
        "# Define a custom stopword list\n",
        "def custom_stopwords():\n",
        "    return set([\n",
        "        \"the\", \"and\", \"a\", \"an\", \"to\", \"of\", \"in\", \"is\", \"it\", \"for\", \"on\",\n",
        "        \"this\", \"by\", \"at\", \"from\", \"or\", \"that\", \"with\", \"as\", \"was\",\n",
        "        \"be\", \"not\", \"are\", \"his\", \"they\", \"he\", \"she\", \"their\", \"you\",\n",
        "        \"we\", \"has\", \"have\", \"had\"\n",
        "    ])\n",
        "\n",
        "# Initialize stopword list and stemmer\n",
        "stop_words = custom_stopwords()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Tokenization, stopword removal, and stemming\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Load documents from Google Drive folder\n",
        "def load_documents(directory):\n",
        "    documents = {}\n",
        "    for filename in sorted(os.listdir(directory)):\n",
        "        if filename.lower().endswith('.pdf'):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            with open(filepath, 'rb') as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "                text = ''\n",
        "                for page in reader.pages:\n",
        "                    text += page.extract_text() or ''\n",
        "                documents[filename] = preprocess_text(text)\n",
        "    return documents\n",
        "\n",
        "# Set the path to your folder inside Google Drive\n",
        "directory = '/content/drive/My Drive/Candidate_list'\n",
        "documents = load_documents(directory)\n",
        "\n",
        "# TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
        "vectorized_docs = vectorizer.fit_transform(documents.values())\n",
        "doc_names = list(documents.keys())\n",
        "\n",
        "# Queries to compare\n",
        "queries = {\n",
        "    \"Query A - Data Management Skills\": \"Understanding of Data Management Principles: Basic knowledge of data governance, data quality, and data lifecycle management Excel & Google Sheets: Proficient in data manipulation, analysis, and visualization.SQL/Python: (Ideally) experience in querying databases and performing data analysis tasks\",\n",
        "    \"Query B - Personal Attributes\": \"Strong focus on data accuracy and precision.Communication Skills: Ability to convey complex information clearly and effectively.Organizational Skills: Capable of managing multiple tasks and priorities efficiently.Logistics & Sup ply Chain: A keen interest in the logistics and supply chain sectors.Business Analytics: Passion for analyzing data to drive business decisions.\",\n",
        "}\n",
        "\n",
        "# Score queries against documents\n",
        "results = {}\n",
        "for query_name, query in queries.items():\n",
        "    query = preprocess_text(query)\n",
        "    query_vector = vectorizer.transform([query])\n",
        "    scores = vectorized_docs.dot(query_vector.T).toarray().flatten()\n",
        "    ranked_indices = np.argsort(scores)[::-1]\n",
        "    ranked_docs = [(doc_names[i], scores[i]) for i in ranked_indices[:8]]\n",
        "    results[query_name] = ranked_docs\n",
        "\n",
        "# Display results\n",
        "for query_name, ranked_docs in results.items():\n",
        "    print(f\"Results for {query_name}:\")\n",
        "    for doc, score in ranked_docs:\n",
        "        print(f\"  Document: {doc}, Score: {score:.4f}\")\n",
        "    print()\n"
      ]
    }
  ]
}